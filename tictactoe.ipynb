{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30152,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook05b607fff2",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaactl/Deep_reinforcement_learning_Course/blob/master/tictactoe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to game bots on Kaggle\n",
        "\n"
      ],
      "metadata": {
        "id": "0i5zhBBaVhvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concepts explored:\n",
        "* MiniMax Algorithm\n",
        "* Q-learning"
      ],
      "metadata": {
        "id": "UTV24AqRVhvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a simple intro for those who are looking to get started with writing bots for two player game setups on kaggle like this one: [ConnectX](https://www.kaggle.com/c/connectx)\n",
        "Competitions like these seemed intimidating, so started with writing bots for a simpler game. Hope this helps those who are stuck on the first step.\n",
        "\n",
        "In this notebook I have touched upon two methods.\n",
        "First the minimax algo, which is a traditional backtracking algorithm. Then the Q-learning algo which is one of the most popular Reinforcement learning methods.\n",
        "\n",
        "#### NoteBook Flow:\n",
        "* First we look at Environment and GYM classes\n",
        "* Then we look at four agents: random, minimax, q-learning, human\n",
        "* Then we mix and match various agents and evaluate their win rates."
      ],
      "metadata": {
        "id": "_7xY2e31Vhvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0 dm-reverb tf-agents"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-27T02:34:17.568751Z",
          "iopub.execute_input": "2024-09-27T02:34:17.569154Z"
        },
        "trusted": true,
        "id": "Xc3VtA1qVhve",
        "outputId": "a7d642df-120f-40f1-8283-60a203adce16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: dm-reverb in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb) (1.5.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents) (10.4.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tf_agents\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import collections\n",
        "import time\n",
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "\n",
        "%config Completer.use_jedi = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:28:03.761215Z",
          "iopub.execute_input": "2022-01-22T21:28:03.761471Z",
          "iopub.status.idle": "2022-01-22T21:28:03.781193Z",
          "shell.execute_reply.started": "2022-01-22T21:28:03.761445Z",
          "shell.execute_reply": "2022-01-22T21:28:03.780239Z"
        },
        "trusted": true,
        "id": "iTr4299OVhvf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment\n",
        "Environment is the setup in which your agent will perform actions. Here the environment will be the tictactoe board which records the actions that the agent took. We must also make sure that the action taken was legal.<br>\n",
        "tf-agents library defines a template that aids us design an environment. We inherit a class call PyEnvironment and override two functions: **_reset** and **_step**<br>\n",
        "For more details: [tf-environments](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial)<br>\n",
        "* Action spec: The list of possible actions an agent can take (here: {x,y} co-ordinate and the player's symbol- '1' for the first player and '2' for the second one)\n",
        "* Observation spec: How the environment is defined. (here a 3x3 board, initialized to 0 to indicate vacant spots)<br>\n",
        "\n",
        "\n",
        "**Overriden functions:**\n",
        "* _reset: defines how the board should look when a game begins\n",
        "* _step: decides how the action affects the board\n",
        "\n",
        "PS: In competitions, this is unnecessary as kaggle provides the environment."
      ],
      "metadata": {
        "id": "jE5Hn8cKVhvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ticTacToe(py_environment.PyEnvironment):\n",
        "    def __init__(self):\n",
        "        self._action_spec = array_spec.BoundedArraySpec((3,), np.int32, 0, 3, 'action')  #[x,y,symbol]\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(3,3), dtype=np.int32, minimum=0, name='observation')\n",
        "        self.board = np.zeros(shape=(3,3), dtype=np.int32)\n",
        "\n",
        "\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "\n",
        "    def _reset(self):\n",
        "        self.board = np.zeros(shape=(3,3), dtype=np.int32)\n",
        "        return ts.restart(np.array(self.board))\n",
        "\n",
        "    def _step(self, action):\n",
        "        '''\n",
        "        Rewards:\n",
        "        -1 for invalid moves\n",
        "        1 if you make the winning move\n",
        "        0 otherwise\n",
        "        '''\n",
        "\n",
        "\n",
        "        index = tuple(action[:-1])\n",
        "\n",
        "        if not self.inrange(index[0],index[1]) or self.board[tuple(index)] != 0:\n",
        "            print(\"INVALID \",action)\n",
        "            return ts.termination(np.array(self.board), -1)\n",
        "\n",
        "        else:\n",
        "            self.board[tuple(index)] = action[-1]\n",
        "            if(self.check_winner(index, action[-1])):\n",
        "                return ts.termination(np.array(self.board), 1)\n",
        "\n",
        "            elif(self.check_draw()):\n",
        "                return ts.termination(np.array(self.board), 0)\n",
        "\n",
        "            else:\n",
        "                return ts.transition(np.array(self.board), 0)\n",
        "\n",
        "    def inrange(self, i,j):\n",
        "        if 0<=i<=2 and 0<=j<=2:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_winner(self, index, player):\n",
        "        '''\n",
        "        Utility function to check if the current move is a winning move\n",
        "        index: (x,y) coordinates of the current move\n",
        "        player: [1|2] symbol of the player who made the current move\n",
        "        '''\n",
        "        x,y = index[0],index[1]\n",
        "\n",
        "        for i in [-1,0,1]:    #for all neighbours of current move\n",
        "            for j in [-1,0,1]:\n",
        "                if i==j==0:    #except itself\n",
        "                    continue\n",
        "\n",
        "                if self.inrange(x+i,y+j):\n",
        "                    if self.board[(x+i,y+j)] == player:    #has the same symbol\n",
        "\n",
        "                        if self.inrange(x+i*2,y+j*2):    #check along the same direction(when you are starting from corners)\n",
        "                            if self.board[(x+i*2,y+j*2)] == player:\n",
        "                                return True\n",
        "\n",
        "\n",
        "                        if self.inrange(x-i,y-j):    #check the other direction(when you are starting from middle)\n",
        "                            if self.board[(x-i,y-j)] == player:\n",
        "                                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "    def check_draw(self):\n",
        "        '''\n",
        "        Utility function to check if the game ended in a draw\n",
        "        Note: check for draw only after checking for a winner\n",
        "        '''\n",
        "        for i in [0,1,2]:\n",
        "            for j in [0,1,2]:\n",
        "                if self.board[(i,j)]==0:\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def display(self):\n",
        "        print(self.board)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:32:56.64781Z",
          "iopub.execute_input": "2022-01-22T21:32:56.648102Z",
          "iopub.status.idle": "2022-01-22T21:32:56.667273Z",
          "shell.execute_reply.started": "2022-01-22T21:32:56.648074Z",
          "shell.execute_reply": "2022-01-22T21:32:56.666128Z"
        },
        "trusted": true,
        "id": "8h-nnyQqVhvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GYM\n",
        "\n",
        "Gym is a wrapper around the environment. This is necessary in a 2 player environment to simulate the second player. It takes an agent as argument which will be used to choose the opponent's action.<br>\n",
        "Any of the agents can be plugged into the gym to train against.<br>\n",
        "\n",
        "Gym encapsulates the env object and handles all interactions between the player agent and the env appropriately.<br>\n",
        "\n",
        "Usage is described in the class<br>\n",
        "\n",
        "\n",
        "PS: in competitions, this is unnecessary"
      ],
      "metadata": {
        "id": "sK-uzSUTVhvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GymTTT():\n",
        "    \"\"\"\n",
        "    It is a wrapper around the tictactoe env.\n",
        "    It takes an agent as argument which will be used to choose the action\n",
        "\n",
        "    Arguments:\n",
        "        agent: instance of an agent\n",
        "        verbose: [True|False] want messages printed to console output?\n",
        "\n",
        "    Usage:\n",
        "        gym = GymTTT(Agent(),True)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, agent, verbose = False):\n",
        "        self.agent = agent\n",
        "        self.env = ticTacToe()\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def reset(self):\n",
        "        if self.agent.symbol == 1:    #if gym's agent has to start, then make the first move\n",
        "            timeStep = self.env.reset()\n",
        "            policyStep = self.agent.action(timeStep)\n",
        "            timeStep = self.env.step(policyStep.action)\n",
        "            timeStep = timeStep._replace(reward = timeStep.reward * -1)    #invert reward\n",
        "\n",
        "\n",
        "            self.print_message(\"\\n********GYM STARTS********\")\n",
        "            self.print_message(\"GYM Agent Move: \"+str(policyStep.action[:2]))\n",
        "            self.fancy_display(timeStep.observation)\n",
        "            return timeStep\n",
        "\n",
        "        else:\n",
        "            self.print_message(\"\\n********PLAYER STARTS********\")\n",
        "            return self.env.reset()\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        #player's move\n",
        "        timeStep = self.env.step(action)\n",
        "        self.print_message(\"Player Move: \"+str(action[:2]))\n",
        "        self.fancy_display(timeStep.observation)\n",
        "\n",
        "        #if player's move was last\n",
        "        if timeStep.is_last():\n",
        "            if timeStep.reward==0:\n",
        "                self.print_message(\"***Game Over: Draw***\")\n",
        "            elif timeStep.reward==-1:\n",
        "                self.print_message(\"***Invalid move: Gym Agent Wins\")\n",
        "            else:\n",
        "                self.print_message(\"*** Yay!: Test Agent Wins ***\")\n",
        "\n",
        "            return timeStep\n",
        "\n",
        "        else:\n",
        "            #agent's move\n",
        "            policyStep = self.agent.action(timeStep)\n",
        "            timeStep = self.env.step(policyStep.action)\n",
        "            timeStep = timeStep._replace(reward = timeStep.reward * -1)    #invert agent's reward for player\n",
        "\n",
        "            self.print_message(\"GYM Agent Move: \"+str(policyStep.action[:2]))\n",
        "            self.fancy_display(timeStep.observation)\n",
        "\n",
        "            if timeStep.is_last():\n",
        "                if timeStep.reward==0:\n",
        "                    self.print_message(\"***Game Over: Draw***\")\n",
        "                else:\n",
        "                    self.print_message(\"***Yay!: Gym Agent Wins ***\")\n",
        "\n",
        "            return timeStep\n",
        "\n",
        "\n",
        "    def print_message(self,message):\n",
        "        if self.verbose:\n",
        "            print(message)\n",
        "\n",
        "\n",
        "    def fancy_display(self,board,action=[-1,-1]):\n",
        "        #array representation was good enough\n",
        "        if self.verbose:\n",
        "            print(board)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:32:59.977778Z",
          "iopub.execute_input": "2022-01-22T21:32:59.978622Z",
          "iopub.status.idle": "2022-01-22T21:32:59.990453Z",
          "shell.execute_reply.started": "2022-01-22T21:32:59.978584Z",
          "shell.execute_reply": "2022-01-22T21:32:59.989298Z"
        },
        "trusted": true,
        "id": "tEJZYRyAVhvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents\n",
        "\n",
        "An agent needs to evaluate the current observation and choose a legal action\n",
        "For uniformity, in this notebook, all agents have a function called action which accepts a timestep as argument and returns a PolicyStep.<br>\n",
        "For more info on the datatypes check out:<br>\n",
        "[tf_agents.trajectories.TimeStep](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/TimeStep)<br>\n",
        "[tf_agents.trajectories.PolicyStep](https://www.tensorflow.org/agents/api_docs/python/tf_agents/trajectories/PolicyStep)<br>\n",
        "\n",
        "An agent needs the player symbol as argument. For simplicity we use 1 and 2 to represent the First and the Second player<br>\n",
        "\n",
        "\n",
        "To start with a simple agent, we can have it choose randomly from a list of empty slots\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6jMfd6pXVhvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomTTTAgent():\n",
        "    \"\"\"\n",
        "         Arguments:\n",
        "            symbol: [1|2] - player symbol\n",
        "        Usage:\n",
        "            Agent =  RandomTTTAgent(1) # Plays first\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,symbol):\n",
        "        self.symbol = symbol\n",
        "        self.trainable = False\n",
        "\n",
        "    def action(self, timestep):\n",
        "        board = timestep.observation\n",
        "        empty_slots = []\n",
        "        for i in range(0,3):\n",
        "            for j in range(0,3):\n",
        "                if board[(i,j)] == 0:\n",
        "                    empty_slots.append([i,j])\n",
        "\n",
        "        choice = random.choice(empty_slots)\n",
        "        return tf_agents.trajectories.PolicyStep(action=choice+[self.symbol], state = board, info=self.symbol)\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:33:02.610702Z",
          "iopub.execute_input": "2022-01-22T21:33:02.610994Z",
          "iopub.status.idle": "2022-01-22T21:33:02.617532Z",
          "shell.execute_reply.started": "2022-01-22T21:33:02.610964Z",
          "shell.execute_reply": "2022-01-22T21:33:02.616431Z"
        },
        "trusted": true,
        "id": "r9HECuw5Vhvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MiniMax\n",
        "Here we implement an agent that chooses the best action by playing out all possible scenarios.<br>\n",
        "For a theoretical understanding, check out [Minimax Algorithm](https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-1-introduction/)<br>\n",
        "In Brief,<br>\n",
        "It builds a tree of all possible actions from the current state and picks the one that fetches the highest reward. To calculate the reward, it simulates the game and plays along all possible sequences. In the simulation, it tries to maximise the reward obatained in its turn and minimise the reward obtained in the opponent's turn."
      ],
      "metadata": {
        "id": "2kgU5wWiVhvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinMaxAgent():    #builds a minmax tree\n",
        "\n",
        "    \"\"\"\n",
        "     Arguments:\n",
        "            symbol: [1|2] - player symbol\n",
        "            verbose: (optional) [True|False] - want messages printed to console output?\n",
        "\n",
        "        Usage:\n",
        "            Agent =  MinMaxAgent(2, True) -- plays first and prints debug messages\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, symbol, verbose=False):\n",
        "        self.symbol = symbol\n",
        "        self.verbose = verbose\n",
        "        self.trainable = False\n",
        "\n",
        "    def action(self, timeStep):\n",
        "        board = np.array(timeStep.observation)\n",
        "\n",
        "        act = self.getBestAction(np.array(board))\n",
        "\n",
        "        return tf_agents.trajectories.PolicyStep(action = list(act)+[self.symbol], state = board, info = self.symbol)\n",
        "\n",
        "\n",
        "    def getBestAction(self,board):\n",
        "        empty_slots = []\n",
        "        for i in range(0,3):\n",
        "            for j in range(0,3):\n",
        "                if board[(i,j)] == 0:\n",
        "                    empty_slots.append([i,j])\n",
        "\n",
        "        best_score = -100\n",
        "        act = ()\n",
        "        temp_board = np.array(board)\n",
        "        if self.verbose:\n",
        "            print(\"MinMax Says:\")\n",
        "        for [i,j] in empty_slots:\n",
        "            temp_board[(i,j)] = self.symbol\n",
        "            score = self.minmax(np.array(temp_board), 0, False, (i,j,self.symbol))\n",
        "            if self.verbose:\n",
        "                print(i,j,score)\n",
        "            if score>best_score:\n",
        "                best_score = score\n",
        "                act = (i,j)\n",
        "            temp_board[(i,j)] = 0\n",
        "\n",
        "        return act\n",
        "\n",
        "\n",
        "    def minmax(self, board, depth, maximise, last_move):\n",
        "        '''\n",
        "            MinMax tree: Alternate between your and opponent's move.\n",
        "            In your move you'll pick the maximising choice; In the opponents move pick the minimising choice(which is his maximum)\n",
        "            Here:\n",
        "                A winning move gets 10 points\n",
        "                A losing move gets -10 points\n",
        "                A drawn board gets 0\n",
        "                All intermediate moves gets (finalScore - numberOfStepsLeft)\n",
        "            Imagine a tree with leaf nodes having 0/10/-10 and with each level above gets max or min of all its branches depending on the level\n",
        "        '''\n",
        "\n",
        "        if self.check_winner(board,last_move):\n",
        "            if last_move[-1] == self.symbol:\n",
        "                return 10\n",
        "            else:\n",
        "                return -10\n",
        "\n",
        "        if self.check_draw(board): #check draw only after checking for winners\n",
        "            return 0\n",
        "\n",
        "\n",
        "        empty_slots = []\n",
        "        for i in range(0,3):\n",
        "            for j in range(0,3):\n",
        "                if board[(i,j)] == 0:\n",
        "                    empty_slots.append([i,j])\n",
        "\n",
        "        temp_board = np.array(board)\n",
        "\n",
        "        if maximise:\n",
        "            best_val = -100\n",
        "            for (i,j) in empty_slots:\n",
        "                player = 3 - last_move[-1]    #switch b/w 1 and 2\n",
        "\n",
        "                temp_board[(i,j)] = player    #pick one of the empty spots\n",
        "\n",
        "                best_val = max(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)),best_val)\n",
        "\n",
        "                temp_board[(i,j)] = 0    #clear the picked empty spot\n",
        "\n",
        "        else:\n",
        "            best_val = 100\n",
        "            for (i,j) in empty_slots:\n",
        "                player = 3 - last_move[-1]\n",
        "                temp_board[(i,j)] = player\n",
        "\n",
        "                best_val = min(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)), best_val)\n",
        "\n",
        "                temp_board[(i,j)] = 0\n",
        "\n",
        "        return best_val\n",
        "\n",
        "\n",
        "\n",
        "    def check_draw(self,board):\n",
        "        board = np.array(board)\n",
        "        for i in [0,1,2]:\n",
        "            for j in [0,1,2]:\n",
        "                if board[(i,j)]==0:\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def check_winner(self, board, last_move):\n",
        "        board = np.array(board)\n",
        "        x,y,player = last_move[0],last_move[1],last_move[2]\n",
        "\n",
        "        for i in [-1,0,1]:    #for all neighbours of current move\n",
        "            for j in [-1,0,1]:\n",
        "                if i==j==0:    #except itself\n",
        "                    continue\n",
        "\n",
        "                if self.inrange(x+i,y+j):\n",
        "                    if board[(x+i,y+j)] == player:    #has the same symbol\n",
        "\n",
        "                        if self.inrange(x+i*2,y+j*2):    #check along the same direction(when you are starting from corners)\n",
        "                            if board[(x+i*2,y+j*2)] == player:\n",
        "                                return True\n",
        "\n",
        "\n",
        "                        if self.inrange(x-i,y-j):    #check the other direction(when you are starting from middle)\n",
        "                            if board[(x-i,y-j)] == player:\n",
        "                                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def inrange(self, i,j):\n",
        "        if 0<=i<=2 and 0<=j<=2:\n",
        "            return True\n",
        "        return False\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:33:07.958058Z",
          "iopub.execute_input": "2022-01-22T21:33:07.95846Z",
          "iopub.status.idle": "2022-01-22T21:33:07.985577Z",
          "shell.execute_reply.started": "2022-01-22T21:33:07.958411Z",
          "shell.execute_reply": "2022-01-22T21:33:07.984256Z"
        },
        "trusted": true,
        "id": "QxLdbdZvVhvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have noticed, minimax algorithm builds a tree in each step even though they are just subtrees of the first computed tree. This excess computation can be avoided by storing the obtained rewards which significantly reduces the compute time of the algorithm."
      ],
      "metadata": {
        "id": "wzx7DFTPVhvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DPMinMaxAgent():    #minmax takes for ever, so memoize it\n",
        "    '''\n",
        "        Here we use a dictionary to store the rewards, where board is the key and its minmax score is its value.\n",
        "        Arguments:\n",
        "            symbol: [1|2] - player symbol\n",
        "            verbose: [True|False] - want messages printed to console output?\n",
        "            saveTree: [True|False] - save the constructed tree dict into a file?\n",
        "            loadTree: [True|False] - load a previously constructed tree dict from memory?\n",
        "            saveTreeFreq: if saveTree is True, save the tree into memory once every 'x' updates. This is coded to decrease over time.\n",
        "\n",
        "        Usage:\n",
        "            Agent = DPMinMaxAgent(2) -- plays second, doesn't save or load from memory\n",
        "            Agent = DPMinMaxAgent(1, verbose=True, saveTree=False, loadTree=True) -- to load from memory but not save it.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, symbol, verbose=False, saveTree=False, loadTree=False, saveTreeFreq=100):\n",
        "        self.symbol = symbol\n",
        "        self.verbose = verbose\n",
        "        self.trainable = False\n",
        "\n",
        "        self.saveTreeFreq = saveTreeFreq\n",
        "        self.saveTreeFreqStart = saveTreeFreq    # high val would mean a lot of them won't be saved; low value would update too many times, so decrease freq periodically\n",
        "        self.saveTree = saveTree\n",
        "        self.pickle_loaded = False\n",
        "        if loadTree:\n",
        "            try:\n",
        "                with open(\"/kaggle/working/minmaxtree.pickle\",\"rb\") as f:\n",
        "                    self.tree = pickle.load(f)\n",
        "                    self.pickle_loaded = True\n",
        "            except Exception as e:\n",
        "                self.pickle_loaded = False\n",
        "                print(e)\n",
        "                loadTree = False\n",
        "\n",
        "        if not self.pickle_loaded:\n",
        "            self.tree = {}\n",
        "\n",
        "\n",
        "    def action(self, timeStep):\n",
        "        board = np.array(timeStep.observation)\n",
        "\n",
        "        act = self.getBestAction(np.array(board))\n",
        "\n",
        "        return tf_agents.trajectories.PolicyStep(action = list(act)+[self.symbol], state = board, info = self.symbol)\n",
        "\n",
        "\n",
        "    def getBestAction(self,board):\n",
        "        empty_slots = []\n",
        "        for i in range(0,3):\n",
        "            for j in range(0,3):\n",
        "                if board[(i,j)] == 0:\n",
        "                    empty_slots.append([i,j])\n",
        "\n",
        "        best_score = -100\n",
        "        act = ()\n",
        "        temp_board = np.array(board)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"MinMax Says:\")\n",
        "\n",
        "        for [i,j] in empty_slots:\n",
        "            temp_board[(i,j)] = self.symbol\n",
        "            score = self.minmax(temp_board, 0, False, (i,j,self.symbol))\n",
        "            if self.verbose:\n",
        "                print(i,j,score)\n",
        "            if score>best_score:\n",
        "                best_score = score\n",
        "                act = (i,j)\n",
        "            temp_board[(i,j)] = 0\n",
        "\n",
        "        return act\n",
        "\n",
        "\n",
        "    def minmax(self, board, depth, maximise, last_move):\n",
        "        last_move = list(last_move)\n",
        "\n",
        "        if self.tree.get(board.tobytes()):    #check dict and return value if it already exists\n",
        "            return self.tree.get(board.tobytes())\n",
        "\n",
        "\n",
        "        if self.check_winner(board,last_move):\n",
        "            if last_move[-1] == self.symbol:\n",
        "                return 20\n",
        "            else:\n",
        "                return -20\n",
        "\n",
        "        if self.check_draw(board):\n",
        "            return 0\n",
        "\n",
        "\n",
        "\n",
        "        empty_slots = []\n",
        "        for i in range(0,3):\n",
        "            for j in range(0,3):\n",
        "                if board[(i,j)] == 0:\n",
        "                    empty_slots.append([i,j])\n",
        "\n",
        "        temp_board = np.array(board)\n",
        "\n",
        "        if maximise:\n",
        "            best_val = -100\n",
        "            for (i,j) in empty_slots:\n",
        "                player = 3 - last_move[-1]\n",
        "\n",
        "                temp_board[(i,j)] = player\n",
        "                temp_board_value = self.oneLess(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)))\n",
        "                if not self.tree.get(temp_board.tobytes()):    #store value if not exists\n",
        "                    self.tree[temp_board.tobytes()] = temp_board_value\n",
        "                    self.saveTreeFreq -= 1\n",
        "\n",
        "                best_val = max(temp_board_value,best_val)\n",
        "                temp_board[(i,j)] = 0\n",
        "\n",
        "        else:\n",
        "            best_val = 100\n",
        "            for (i,j) in empty_slots:\n",
        "                player = 3 - last_move[-1]\n",
        "\n",
        "                temp_board[(i,j)] = player\n",
        "                temp_board_value = self.oneLess(self.minmax(np.array(temp_board), depth+1, not maximise, (i,j,player)))\n",
        "                if not self.tree.get(temp_board.tobytes()):\n",
        "                    self.tree[temp_board.tobytes()] = temp_board_value\n",
        "                    self.saveTreeFreq -= 1\n",
        "\n",
        "                best_val = min(temp_board_value, best_val)\n",
        "                temp_board[(i,j)] = 0\n",
        "\n",
        "\n",
        "        if self.saveTree and self.saveTreeFreq==0:\n",
        "            with open(\"/kaggle/working/minmaxtree.pickle\",\"wb\") as f:\n",
        "                pickle.dump(self.tree, f)\n",
        "                self.saveTreeFreq = self.saveTreeFreqStart\n",
        "                self.saveTreeFreqStart -= 1    # n*(n-1)/2 updates\n",
        "\n",
        "        return best_val\n",
        "\n",
        "\n",
        "    def oneLess(self,x):\n",
        "        if x>0:\n",
        "            return x-1\n",
        "        elif x< 0:\n",
        "            return x+1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "    def check_draw(self,board):\n",
        "        board = np.array(board)\n",
        "        for i in [0,1,2]:\n",
        "            for j in [0,1,2]:\n",
        "                if board[(i,j)]==0:\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def check_winner(self, board, last_move):\n",
        "        board = np.array(board)\n",
        "        x,y,player = last_move[0],last_move[1],last_move[2]\n",
        "\n",
        "        for i in [-1,0,1]:    #for all neighbours of current move\n",
        "            for j in [-1,0,1]:\n",
        "                if i==j==0:    #except itself\n",
        "                    continue\n",
        "\n",
        "                if self.inrange(x+i,y+j):\n",
        "                    if board[(x+i,y+j)] == player:    #has the same symbol\n",
        "\n",
        "                        if self.inrange(x+i*2,y+j*2):    #check along the same direction(when you are starting from corners)\n",
        "                            if board[(x+i*2,y+j*2)] == player:\n",
        "                                return True\n",
        "\n",
        "\n",
        "                        if self.inrange(x-i,y-j):    #check the other direction(when you are starting from middle)\n",
        "                            if board[(x-i,y-j)] == player:\n",
        "                                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def inrange(self, i,j):\n",
        "        if 0<=i<=2 and 0<=j<=2:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:33:12.151499Z",
          "iopub.execute_input": "2022-01-22T21:33:12.151936Z",
          "iopub.status.idle": "2022-01-22T21:33:12.17845Z",
          "shell.execute_reply.started": "2022-01-22T21:33:12.15189Z",
          "shell.execute_reply": "2022-01-22T21:33:12.177453Z"
        },
        "trusted": true,
        "id": "QZp5XkCdVhvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test out the minimax agent against the random agent<br>"
      ],
      "metadata": {
        "id": "ye5aZX-qVhvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = time.time()\n",
        "\n",
        "TestAgent = DPMinMaxAgent(1)    #Agent 1\n",
        "gymAgent = RandomTTTAgent(2)    #Agent 2\n",
        "\n",
        "gym = GymTTT(gymAgent,True)\n",
        "\n",
        "timeStep = gym.reset()\n",
        "\n",
        "while not timeStep.is_last():\n",
        "    timeStep = gym.step(TestAgent.action(timeStep).action)\n",
        "\n",
        "print(time.time() - t)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:33:17.592334Z",
          "iopub.execute_input": "2022-01-22T21:33:17.592766Z",
          "iopub.status.idle": "2022-01-22T21:33:19.189185Z",
          "shell.execute_reply.started": "2022-01-22T21:33:17.592733Z",
          "shell.execute_reply": "2022-01-22T21:33:19.187782Z"
        },
        "trusted": true,
        "id": "7C1fhZB0Vhvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to evaluate an agent, its best to play against it yourself. So use a Human agent if you want to  play against one of the other agents."
      ],
      "metadata": {
        "id": "_4n9rGBoVhvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HumanTTTAgent():    #for humans vs machine\n",
        "    def __init__(self, symbol, verbose=False):\n",
        "        self.symbol= symbol\n",
        "        self.verbose = verbose\n",
        "        self.trainable = False\n",
        "\n",
        "\n",
        "    def action(self, timeStep):\n",
        "        board = timeStep.observation\n",
        "\n",
        "        empty_slots = []\n",
        "        for i in range(0,3):\n",
        "            for j in range(0,3):\n",
        "                if board[(i,j)] == 0:\n",
        "                    empty_slots.append([i,j])\n",
        "\n",
        "        print(board)\n",
        "        print(\"input space seperated indices; choose from the empty slots\")\n",
        "        print(\"EmptySlots: \",empty_slots)\n",
        "        i,j = self.get_inputs()\n",
        "        tries = 2\n",
        "        while [i,j] not in empty_slots and tries:\n",
        "            print(\"invalid choice, input space seperated indices, tries left: \",tries)\n",
        "            i,j = self.get_inputs()\n",
        "            tries -= 1\n",
        "\n",
        "        if not tries:\n",
        "            print(\"Illiterate\")\n",
        "\n",
        "        act = [i,j,self.symbol]\n",
        "        return tf_agents.trajectories.PolicyStep(action = act, state = board, info = self.symbol)\n",
        "\n",
        "    def get_inputs(self):\n",
        "        try:\n",
        "            i,j = [int(x) for x in input().split()]\n",
        "            return (i,j)\n",
        "        except:\n",
        "            return (9,9)\n",
        "\n",
        "    def updateActionValue(self, qtuple):\n",
        "        pass"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:33:24.459328Z",
          "iopub.execute_input": "2022-01-22T21:33:24.459727Z",
          "iopub.status.idle": "2022-01-22T21:33:24.470818Z",
          "shell.execute_reply.started": "2022-01-22T21:33:24.459679Z",
          "shell.execute_reply": "2022-01-22T21:33:24.470012Z"
        },
        "trusted": true,
        "id": "ONywHrdsVhvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use this as a template to play around with the agents\n",
        "'''\n",
        "t = time.time()\n",
        "\n",
        "TestAgent =     #Agent 1\n",
        "gymAgent =     #Agent 2\n",
        "\n",
        "gym = GymTTT(gymAgent,True)\n",
        "\n",
        "timeStep = gym.reset()\n",
        "\n",
        "while not timeStep.is_last():\n",
        "    timeStep = gym.step(TestAgent.action(timeStep).action)\n",
        "\n",
        "print(time.time() - t)\n",
        "'''"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:33:57.201002Z",
          "iopub.execute_input": "2022-01-22T21:33:57.201368Z",
          "iopub.status.idle": "2022-01-22T21:34:14.439658Z",
          "shell.execute_reply.started": "2022-01-22T21:33:57.201272Z",
          "shell.execute_reply": "2022-01-22T21:34:14.438798Z"
        },
        "trusted": true,
        "id": "aZSIS8V2Vhvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning\n",
        "\n",
        "Here we implement an agent which uses Q-learning to choose actions. Its a classic reinforcement technique that determines the action based on its past experiences.<br>\n",
        "For a structured theoretical understanding please refer to [Suton and Barto](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).<br>\n",
        "In brief, it relies on its experience to determine how good is an action at a certain stage of the game, which is quantified by reward. For each step of the game, we store the reward obtained and this value is corrected each time we visit this state using the q-learning formula:<br>\n",
        "![Q-Learning-Formula](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/1_lTVHyzT3d26Bd_znaKaylQ.png)<br>\n",
        "Then we pick the action which has the highest reward associated with it, the next time we are in the same state.\n",
        "Here we use a dictionary to store and update these action values. Since this involves training, I have already attached a trained dictionary which can optionally be loaded by setting the load_av argument to true.<br>"
      ],
      "metadata": {
        "id": "VC__chg5Vhvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent():\n",
        "    def __init__(self, symbol, trainable = True, alpha = 0.1, gamma =1, load_av = False, save_av = False, saveAVFreq = 100):\n",
        "        '''\n",
        "        Action values: Dictionary with board as index and a dictionary of {state:{action:reward}} as values\n",
        "\n",
        "        Arguments:\n",
        "            symbol: [1|2] - player symbol\n",
        "            trainable: [True|False] - should we use the current game for training?\n",
        "            alpha: learning rate\n",
        "            gamma: discount rate\n",
        "            load_av: [True|False] - load a previously trained dict from memory?\n",
        "            save_av: [True|False] - save action values into memory?\n",
        "            saveAVFreq: save action values after these many updates\n",
        "\n",
        "        Usage:\n",
        "            Agent = QLearningAgent(2) -- plays second, doesn't save or load from memory\n",
        "            Agent = QLearningAgent(1, save_av=False, load_av=True) -- load trained data from memory but don't save any updates.\n",
        "            Agent = QLearningAgent(1, alpha=0.3, gamma=0.9)\n",
        "\n",
        "        '''\n",
        "        self.symbol = symbol\n",
        "        self.trainable = trainable\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.saveAVFreq = saveAVFreq\n",
        "        self.save_av = save_av\n",
        "        self.pickle_loaded = False\n",
        "        if load_av:\n",
        "            try:\n",
        "                with open(\"/kaggle/working/av.pickle\",\"rb\") as f:\n",
        "                    self.av = pickle.load(f)\n",
        "                    self.pickle_loaded = True\n",
        "            except Exception as e:\n",
        "                self.pickle_loaded = False\n",
        "                print(e)\n",
        "                load_av = False\n",
        "\n",
        "        if not self.pickle_loaded:\n",
        "            self.av = collections.defaultdict(self.module_default_dict)\n",
        "\n",
        "\n",
        "    def module_default_dict(self):    #pickle cant save lambdas\n",
        "        return collections.defaultdict(int)\n",
        "\n",
        "    def updateActionValue(self, qtuple):\n",
        "        '''\n",
        "            qtuple holds:\n",
        "                current state: board - 3x3 array\n",
        "                action taken: indices - [i,j]\n",
        "                reward obtained\n",
        "                next state: board - 3x3 array\n",
        "        '''\n",
        "\n",
        "        current = qtuple['cur'].tobytes() #serialize it\n",
        "        action = qtuple['act'] if len(qtuple['act']) == 2 else qtuple['act'][:2]\n",
        "        reward = qtuple['rew']\n",
        "        nextState = qtuple['nex'].tobytes()\n",
        "\n",
        "        qmax = max(self.av[(nextState,self.symbol)].values()) if self.av[(nextState,self.symbol)].values() else 0    #if next_state is not present in av, then return 0\n",
        "        self.av[(current,self.symbol)][tuple(action)] += self.alpha * (reward + self.gamma * qmax - self.av[(current,self.symbol)][tuple(action)])\n",
        "\n",
        "        self.saveAVFreq -= 1\n",
        "        if self.save_av and self.saveAVFreq == 0:\n",
        "            with open(\"/kaggle/working/av.pickle\",\"wb\") as f:\n",
        "                pickle.dump(self.av, f)\n",
        "            self.saveAVFreq = 100\n",
        "\n",
        "\n",
        "    def action(self, timeStep):\n",
        "        board = timeStep.observation\n",
        "        qsa = dict(self.av[(board.tobytes(),self.symbol)])\n",
        "\n",
        "        was_random_choice = False\n",
        "        empty_slots = []    #find the list of actions possible\n",
        "        for i in range(0,3):\n",
        "            for j in range(0,3):\n",
        "                if board[(i,j)] == 0:\n",
        "                    empty_slots.append([i,j])\n",
        "\n",
        "        while qsa and max(qsa.values())>=0:  #choose from the existing options only if it has a non -ve value\n",
        "            act = max(qsa, key=qsa.get)\n",
        "            if board[tuple(act)] ==0:\n",
        "                break\n",
        "            qsa.pop(act,None)\n",
        "\n",
        "        else:\n",
        "            was_random_choice = True\n",
        "            act = random.choice(empty_slots)    #else chose randomly\n",
        "\n",
        "\n",
        "        return tf_agents.trajectories.PolicyStep(action=list(act) + [self.symbol], state = board, info=was_random_choice)\n",
        "\n",
        "\n",
        "    def displayAV(self):\n",
        "        print(\"***AV***\")\n",
        "        for k,v in dict(self.av).items():\n",
        "            board = np.ndarray((3,3), np.int32, k[0])\n",
        "            print(board,k[1])\n",
        "            print(v)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T22:24:48.104206Z",
          "iopub.execute_input": "2022-01-22T22:24:48.104519Z",
          "iopub.status.idle": "2022-01-22T22:24:48.121261Z",
          "shell.execute_reply.started": "2022-01-22T22:24:48.104483Z",
          "shell.execute_reply": "2022-01-22T22:24:48.120334Z"
        },
        "trusted": true,
        "id": "4f9PS1DuVhvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity test against a random agent\n",
        "t = time.time()\n",
        "\n",
        "TestAgent = QLearningAgent(1)    #Agent 1\n",
        "gymAgent = RandomTTTAgent(2)    #Agent 2\n",
        "\n",
        "gym = GymTTT(gymAgent,True)\n",
        "\n",
        "timeStep = gym.reset()\n",
        "\n",
        "while not timeStep.is_last():\n",
        "    timeStep = gym.step(TestAgent.action(timeStep).action)\n",
        "\n",
        "print(time.time() - t)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T21:34:41.095106Z",
          "iopub.execute_input": "2022-01-22T21:34:41.095367Z",
          "iopub.status.idle": "2022-01-22T21:34:41.106649Z",
          "shell.execute_reply.started": "2022-01-22T21:34:41.095339Z",
          "shell.execute_reply": "2022-01-22T21:34:41.10589Z"
        },
        "trusted": true,
        "id": "58MilfjyVhvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An untrained Q-learning agent would pick random actions. With each game, it learns to differentiate good moves from bad ones. So let us train our agent.<br>\n",
        "As we train, it is imperitive to collect your game results as well. Its the best metric to track the training.<br>\n",
        "As I had mentioned earlier, the q-learning agent makes a random choice when it hasn't seen the current state before, hence measuring the number of random actions vs choosen actions could also serve as a metric to track the training levels.<br>\n",
        "We can choose any of the agents(including itself) as the opponent.<br>\n",
        "\n",
        "\n",
        "Note: The agent must be trained as both First Player and Second Player"
      ],
      "metadata": {
        "id": "_7_lXf9RVhvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stats\n",
        "results = []\n",
        "random_actions = []"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T23:32:21.398132Z",
          "iopub.execute_input": "2022-01-22T23:32:21.398428Z",
          "iopub.status.idle": "2022-01-22T23:32:21.402417Z",
          "shell.execute_reply.started": "2022-01-22T23:32:21.398395Z",
          "shell.execute_reply": "2022-01-22T23:32:21.401558Z"
        },
        "trusted": true,
        "id": "UkjfuY6SVhvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the agent to be train\n",
        "TestAgent = QLearningAgent(1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T23:32:22.867565Z",
          "iopub.execute_input": "2022-01-22T23:32:22.868111Z",
          "iopub.status.idle": "2022-01-22T23:32:22.871831Z",
          "shell.execute_reply.started": "2022-01-22T23:32:22.868075Z",
          "shell.execute_reply": "2022-01-22T23:32:22.871037Z"
        },
        "trusted": true,
        "id": "s2dDFiV7Vhvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize gym\n",
        "gymAgent = RandomTTTAgent(2)\n",
        "gym = GymTTT(gymAgent,False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T23:32:29.196686Z",
          "iopub.execute_input": "2022-01-22T23:32:29.196956Z",
          "iopub.status.idle": "2022-01-22T23:32:29.201972Z",
          "shell.execute_reply.started": "2022-01-22T23:32:29.196913Z",
          "shell.execute_reply": "2022-01-22T23:32:29.200956Z"
        },
        "trusted": true,
        "id": "sRGYoGzUVhvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "games = 1000    #episodes\n",
        "for i in range(games):\n",
        "    timeStep = gym.reset()\n",
        "    random_action_count = 0\n",
        "    while not timeStep.is_last():\n",
        "        policyStep = TestAgent.action(timeStep)\n",
        "        nexTimeStep = gym.step(policyStep.action)\n",
        "\n",
        "        if (policyStep.info):\n",
        "            random_action_count+=1\n",
        "\n",
        "        qtuple = {}\n",
        "        qtuple['cur'] = timeStep.observation\n",
        "        qtuple['act'] = policyStep.action[:2]\n",
        "        qtuple['rew'] = nexTimeStep.reward\n",
        "        qtuple['nex'] = nexTimeStep.observation\n",
        "\n",
        "        if TestAgent.trainable:\n",
        "            TestAgent.updateActionValue(qtuple)\n",
        "\n",
        "        timeStep = nexTimeStep\n",
        "\n",
        "    results.append(int(timeStep.reward))    #collect stats\n",
        "    random_actions.append(random_action_count)\n",
        "\n",
        "\n",
        "print(len(results))\n",
        "print(time.time() - start)\n",
        "print(\"Win Percentage: \",results[-games:].count(1)/games)    #stats for last x games\n",
        "print(\"Draw Percentage: \",results[-games:].count(0)/games)\n",
        "print(\"Loss Percentage: \",results[-games:].count(-1)/games)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T23:32:31.400856Z",
          "iopub.execute_input": "2022-01-22T23:32:31.401591Z",
          "iopub.status.idle": "2022-01-22T23:32:32.200454Z",
          "shell.execute_reply.started": "2022-01-22T23:32:31.401552Z",
          "shell.execute_reply": "2022-01-22T23:32:32.199491Z"
        },
        "trusted": true,
        "id": "f8MDi_mfVhvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the metrics"
      ],
      "metadata": {
        "id": "BDi22T2_Vhvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq = 50    #bucket-size for plotting higher sizes give smoother curves\n",
        "\n",
        "a,b = 0, len(results)    #range to plot; (change a to len(results)-x to plot only the last x)\n",
        "\n",
        "metrics = collections.defaultdict(list)\n",
        "\n",
        "for i in range(a, b, freq):\n",
        "    if i==0:\n",
        "        continue\n",
        "\n",
        "    metrics['games'].append(i)\n",
        "    metrics['wins'].append(results[i-freq:i].count(1))\n",
        "    metrics['draws'].append(results[i-freq:i].count(0))\n",
        "    metrics['loses'].append(results[i-freq:i].count(-1))\n",
        "    metrics['win_pct'].append(results[i-freq:i].count(1)/float(freq))\n",
        "\n",
        "metrics['randomness'] = [sum(random_actions[i-freq:i]) for i in range(a, b,freq) if i!=0] #how many of the actions were just random(by a q agent)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(metrics['games'], metrics['wins'], label='wins')\n",
        "# plt.plot(metrics['games'], metrics['win_pct'], label='wins_pct')\n",
        "plt.plot(metrics['games'], metrics['draws'], label='draws')\n",
        "plt.plot(metrics['games'], metrics['loses'], label='loses')\n",
        "# plt.plot(metrics['games'], metrics['randomness'], label='randomness')\n",
        "plt.legend(loc = 0)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T23:32:35.003876Z",
          "iopub.execute_input": "2022-01-22T23:32:35.004193Z",
          "iopub.status.idle": "2022-01-22T23:32:35.213757Z",
          "shell.execute_reply.started": "2022-01-22T23:32:35.004159Z",
          "shell.execute_reply": "2022-01-22T23:32:35.212937Z"
        },
        "trusted": true,
        "id": "IYh6HTAoVhvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}